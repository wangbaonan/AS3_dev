#!/share/apps/cluster/anaconda3-2022.10/envs/as3_mamba/bin/python3
import argparse
import pickle
import time
import os
import sys
import logging
import random
import numpy as np
from pathlib import Path
from datetime import datetime, timezone, timedelta

import torch
from torch.utils.data import DataLoader
import warnings
import pysam

from src.dataloaders import ReferencePanelDataset, ChunkedReferencePanelDataset, reference_panel_collate
from src.models import AgnosticModelInferNoSmoother, AncestryLevelConvSmoother
from src.stepsagnostic import inference_and_write, build_transforms

def set_global_seed(seed):
    """
    设置一个全局随机种子以确保代码的可复现性。
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True

# 立即调用！
SEED = 320
set_global_seed(SEED)
print(f"--- GLOBAL SEED SET TO {SEED} AT SCRIPT START ---")


class LocalTimeFormatter(logging.Formatter):
    CST = timezone(timedelta(hours=8))

    def formatTime(self, record, datefmt=None):
        utc_dt = datetime.fromtimestamp(record.created, tz=timezone.utc)
        local_dt = utc_dt.astimezone(self.CST)
        if datefmt:
            return local_dt.strftime(datefmt)
        else:
            return local_dt.strftime("%Y-%m-%d %H:%M:%S,%f")[:-3]

def setup_logging(log_folder):
    log_filename = os.path.join(log_folder, "run.log")
    log_formatter = LocalTimeFormatter("%(asctime)s [%(levelname)-5.5s]  %(message)s")
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    if root_logger.hasHandlers():
        root_logger.handlers.clear()
    file_handler = logging.FileHandler(log_filename)
    file_handler.setFormatter(log_formatter)
    root_logger.addHandler(file_handler)
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(log_formatter)
    root_logger.addHandler(console_handler)

def setup_arg_parser():
    parser = argparse.ArgumentParser(description="ArchaicSeeker3.0 - Professional Edition")

    BASE_DIR = Path(__file__).resolve().parent
    BASE_MODEL_PATH = BASE_DIR / "exp" / "Basemodel_Mamba_4096" / "models" / "best_model.pth"
    SMOOTHER_MODEL_PATH = BASE_DIR / "exp" / "Smoother_4096_512_Kernel_4096" / "models" / "best_model.pth"

    parser.add_argument("--base-model-cp", type=str, default=BASE_MODEL_PATH, help="Path to the base model checkpoint file.")
    parser.add_argument("--base-model-args", type=str, default=None, help="Path to the base model arguments pckl file. If None, it's derived from the checkpoint path.")
    parser.add_argument("--smoother-model-cp", type=str, default=SMOOTHER_MODEL_PATH, help="Path to the smoother model checkpoint file.")
    parser.add_argument("--smoother-model-args", type=str, default=None, help="Path to the smoother model arguments pckl file. If None, it's derived from the checkpoint path.")
    
    parser.add_argument("--stride", type=int, default=512, help="Window stride for inference.")
    parser.add_argument("--merge", type=int, default=5000, help="Merge threshold for segments, default: 5000bp.")
    parser.add_argument("--anc", type=int, default=0, help="Archaic parameter setting.")

    parser.add_argument("--test-mixed", '-t', type=str, required=True, help="Path to the PHASED test mixed VCF file (Required).")
    parser.add_argument("--reference", '-r', type=str, required=True, help="Path to reference VCF file (Archaic and African) (Required).")
    parser.add_argument("--map", '-m', type=str, required=True, help="Path to the reference map file (Required).")
    parser.add_argument('--out-folder', '-o', type=str, required=True, help="Output folder for the results (Required).")
    
    parser.add_argument("--target-chunk-size", type=int, default=None, 
                        help="Process target samples in chunks of this size to save memory. If None, all samples are processed at once. (e.g., 10)")
    
    return parser

def check_required_files(args):
    logging.info("Checking for existence of required input files...")
    required_files = [
        args.test_mixed, 
        args.reference, 
        args.map, 
        args.base_model_cp, 
        args.smoother_model_cp
    ]
    for f_path in required_files:
        if not os.path.exists(f_path):
            logging.error(f"Required input file not found: {f_path}")
            sys.exit(1) 
    logging.info("All required files found.")

def load_and_prepare_args(args):
    if not args.base_model_args:
        base_model_cp_str = str(args.base_model_cp)
        args.base_model_args = base_model_cp_str.replace('models/best_model.pth', 'args.pckl').replace('models/last_model.pth', 'args.pckl')
    
    if not args.smoother_model_args:
        smoother_model_cp_str = str(args.smoother_model_cp)
        args.smoother_model_args = smoother_model_cp_str.replace('models/best_model.pth', 'args.pckl').replace('models/last_model.pth', 'args.pckl')

    logging.info(f"Loading base model arguments from: {args.base_model_args}")
    try:
        with open(args.base_model_args, "rb") as f:
            base_model_args = pickle.load(f)
            base_model_args.win_stride = args.stride
    except FileNotFoundError:
        logging.error(f"Argument file not found: {args.base_model_args}")
        sys.exit(1)
    except Exception as e:
        logging.error(f"Failed to load or process arguments from {args.base_model_args}: {e}")
        sys.exit(1)

    logging.info(f"Loading smoother model arguments from: {args.smoother_model_args}")
    try:
        with open(args.smoother_model_args, "rb") as f:
            smoother_model_args = pickle.load(f)
            smoother_model_args.win_stride = args.stride
    except FileNotFoundError:
        logging.error(f"Argument file not found: {args.smoother_model_args}")
        sys.exit(1)
    except Exception as e:
        logging.error(f"Failed to load or process arguments from {args.smoother_model_args}: {e}")
        sys.exit(1)
        
    return base_model_args, smoother_model_args

def load_models(base_model_cp, smoother_model_cp, base_model_args, device):
    logging.info("Loading BaseModel...")
    try:
        baseModel = AgnosticModelInferNoSmoother(base_model_args)
        base_original_state_dict = torch.load(base_model_cp, map_location=device)
        baseModel.load_state_dict(base_original_state_dict)
        baseModel = baseModel.to(device)
        baseModel.eval()
    except Exception as e:
        logging.error(f"Failed to load BaseModel from {base_model_cp}: {e}")
        sys.exit(1)

    logging.info("Loading SmootherModel...")
    try:
        smootherModel = AncestryLevelConvSmoother(in_planes=4, planes=32, kernel_size=8192)
        smoother_original_state_dict = torch.load(smoother_model_cp, map_location=device)
        smootherModel.load_state_dict(smoother_original_state_dict)
        smootherModel = smootherModel.to(device)
        smootherModel.eval()
    except Exception as e:
        logging.error(f"Failed to load SmootherModel from {smoother_model_cp}: {e}")
        sys.exit(1)
        
    return baseModel, smootherModel

def main():
    
    start_time = time.time()

    parser = setup_arg_parser()
    args = parser.parse_args()

    os.makedirs(args.out_folder, exist_ok=True)
    setup_logging(args.out_folder)
    
    warnings.filterwarnings("ignore", category=UserWarning)

    logging.info("======================================================")
    logging.info("               ArchaicSeeker 3.1 Mamba Started              ")
    logging.info("======================================================")
    logging.info(f"All logs will be saved to: {os.path.join(args.out_folder, 'run.log')}")
    
    check_required_files(args)
    
    base_model_args, smoother_model_args = load_and_prepare_args(args)
    logging.info(f"Base model args loaded: {vars(base_model_args)}")
    logging.info(f"Smoother model args loaded: {vars(smoother_model_args)}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"Using device: {device}")
    if os.environ.get("CUDA_VISIBLE_DEVICES"):
        logging.info(f"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")
    
    baseModel, smootherModel = load_models(args.base_model_cp, args.smoother_model_cp, base_model_args, device)
    logging.info("All models loaded successfully.")

    logging.info(f"Reading sample list from target VCF: {args.test_mixed}")
    try:
        with pysam.VariantFile(args.test_mixed) as vcf_file:
            all_target_samples = list(vcf_file.header.samples)
        if not all_target_samples:
            raise ValueError("No samples found in the target VCF file.")
    except Exception as e:
        logging.error(f"Failed to read sample names from target VCF {args.test_mixed}: {e}", exc_info=True)
        sys.exit(1)

    logging.info("Creating complete haplotype ID to sample name map...")
    hap_id_to_name_map = {
        i: f"{all_target_samples[i // 2]}_{i % 2 + 1}" 
        for i in range(len(all_target_samples) * 2)
    }

    total_samples = len(all_target_samples)
    chunk_size = args.target_chunk_size if args.target_chunk_size and args.target_chunk_size > 0 else total_samples
    num_chunks = (total_samples + chunk_size - 1) // chunk_size
    
    logging.info(f"Total target samples: {total_samples}. Processing in {num_chunks} chunk(s) of size {chunk_size}.")

    output_txt_path = os.path.join(args.out_folder, 'introgression_prediction.txt')
    output_bed_path = os.path.join(args.out_folder, 'introgression_prediction.bed')
    
    open(output_txt_path, 'w').close()
    open(output_bed_path, 'w').close()

    current_index_counter = 0
    current_index_filter_counter = 0

    # OPTIMIZATION: Initialize dataset once with shared data (reference panel, positions, alignment)
    # This avoids redundant loading and alignment for each chunk
    genetic_map = getattr(args, 'genetic_map', None)

    if num_chunks > 1:
        logging.info("Using ChunkedReferencePanelDataset for efficient multi-chunk processing...")
        try:
            test_dataset = ChunkedReferencePanelDataset(
                mixed_file_path=args.test_mixed,
                reference_panel_vcf=args.reference,
                reference_panel_map=args.map,
                n_refs_per_class=base_model_args.n_refs,
                transforms=build_transforms(base_model_args),
                genetic_map=genetic_map,
                single_arc=args.anc
            )
        except Exception as e:
            logging.error(f"Failed to initialize ChunkedReferencePanelDataset. Aborting.", exc_info=True)
            sys.exit(1)

    for i in range(num_chunks):
        chunk_start_index = i * chunk_size
        chunk_end_index = chunk_start_index + chunk_size
        sample_chunk_list = all_target_samples[chunk_start_index:chunk_end_index]

        logging.info(f"--- Processing Chunk {i+1}/{num_chunks} (Samples {chunk_start_index+1} to {min(chunk_end_index, total_samples)}) ---")

        try:
            if num_chunks > 1:
                # Use the optimized chunked dataset - only load sample data
                test_dataset.load_sample_chunk(sample_chunk_list)
            else:
                # Single chunk - use original implementation
                logging.info("Single chunk detected. Using standard ReferencePanelDataset...")
                test_dataset = ReferencePanelDataset(
                    mixed_file_path=args.test_mixed,
                    samples_to_load=sample_chunk_list,
                    reference_panel_vcf=args.reference,
                    reference_panel_map=args.map,
                    n_refs_per_class=base_model_args.n_refs,
                    transforms=build_transforms(base_model_args),
                    genetic_map=genetic_map,
                    single_arc=args.anc
                )

            test_loader = DataLoader(test_dataset, batch_size=1, collate_fn=reference_panel_collate, shuffle=False, num_workers=0)
            info = test_dataset.info

            current_index_counter, current_index_filter_counter = inference_and_write(
                baseModel, smootherModel, test_loader, args,
                output_txt_path, output_bed_path, info,
                hap_id_to_name_map,
                is_first_chunk=(i == 0),
                current_index=current_index_counter,
                current_index_filter=current_index_filter_counter
            )
        except Exception as e:
            logging.error(f"An error occurred while processing chunk {i+1}. Aborting.", exc_info=True)
            sys.exit(1)

    logging.info("All chunks processed successfully.")
    logging.info(f"Final output files are located in: {args.out_folder}")
    
    total_time = time.time() - start_time
    duration_str = str(timedelta(seconds=int(total_time)))
    logging.info("======================================================")
    logging.info(f"      Application finished in {duration_str}      ")
    logging.info("======================================================")

if __name__ == '__main__':
    main()
